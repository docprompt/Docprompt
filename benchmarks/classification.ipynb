{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Page Classification\n",
                "\n",
                "A very important task for PDF and document processing is page classification. This task is often used as the first step in data ingestion pipelines, as it allows us to clearly classify and label how we will need to perform further processing steps on our various pages, as well as tag them with important pieces of metadata."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Environment Setup\n",
                "\n",
                "To start, let's make sure the environment is setup correctly. Depending on what service provider you are using, there are a few environment variables you will need to set or you may choose to pass the credentials as kwargs at run-time instead.\n",
                "\n",
                "**For Anthropic**:\n",
                "- `ANTHROPIC_API_KEY`: The API key for your anthropic account"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv(\"../.env\")\n",
                "\n",
                "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", None)\n",
                "assert ANTHROPIC_API_KEY is not None, \"ANTHROPIC_API_KEY is not set\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load Document Resources\n",
                "\n",
                "With our environment properly configured, we can now begin loading our PDF documents into the environment. There are a few unique ways to store and retrieve documents, but we will opt for the simplest, out of the box method, which is the `load_document_node` utility.\n",
                "\n",
                "For this exercise, we will be loading a PDF file of a legal deposition. It is around ~40 pages long and contains the deposition transcript as well as some additional title, index, and metadata pages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from docprompt import load_document_node\n",
                "\n",
                "# A PDF with a legal deposition\n",
                "node = load_document_node(\"../data/example-1.pdf\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Setup Classification Task\n",
                "\n",
                "A first step in many document processing pipelines, is to classify all of the individual document pages that we are ingesting. This first step often ends up being some form of page classification, be it binary, single, or multi-label. Regardless, it is a curcial step in nearly every document processing workflow.\n",
                "\n",
                "To begin, let's use the `ClassificationConfig` class to setup the parameters of our classification task."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from docprompt.tasks.classification.base import ClassificationConfig\n",
                "\n",
                "# Setup our classification task\n",
                "singel_label_config = ClassificationConfig(\n",
                "    # Declare the task type as 'single_label' for single-label classification\n",
                "    type='single_label',\n",
                "\n",
                "    # Define the label categories for classification task\n",
                "    labels=['title_page', 'index_page', 'body_page', \"other_page\"],\n",
                "\n",
                "    # Add your own custom instructions for the model, if you find it needs additional guidance for your domain\n",
                "    instructions=\"Classify the page of the legal deposition carefully. Be sure to read the page carefully and select the most appropriate label. If you are unsure, select 'other_page'.\",\n",
                "\n",
                "    # Provide the model with detailed descriptions of each label category (optional -- but reccomended)\n",
                "    descriptions=[\n",
                "        \"A title page of the deposition, containing the title, participants, and other metadata.\",\n",
                "        'An index page of the deposition, containing a table of contents or other reference aids.',\n",
                "        \"A page containing the transcript or dialgoue of the deposition.\",\n",
                "        \"Any other page in the deposition, that doesn't fit into the other categories.\"\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Execute Classification Task\n",
                "\n",
                "Now that we have our classification task configured, we need to use the Anthropic Factory to create an Anthropic Page Classification Provider."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from docprompt.tasks.factory import AnthropicTaskProviderFactory\n",
                "\n",
                "factory = AnthropicTaskProviderFactory()\n",
                "classification_provider = factory.get_page_classification_provider()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing messages: 42it [00:03, 11.17it/s]\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Run the page classification\n",
                "single_label_results = await classification_provider.aprocess_document_node(\n",
                "    node, # The document node to process\n",
                "    task_config=singel_label_config # Pass classification config at runtime\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for page_num, result in single_label_results.items():\n",
                "    print(f\"Page {page_num}: {result.labels}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can see that our classification results seem to make a lot of sense. The model identified a few title pages at the begining of the document, an index page at page 3 (likely an table of contents, note of exhibits, etc.) and then all body pages up until the final page of the document. \n",
                "\n",
                "### Another Approach for the Same Task\n",
                "\n",
                "While single label classification certainly applies well to this task, we can also see the use case where we only want to differentiate between body and non-body pages. In this instance, we could use a binary classification task (which will be more token efficient and faster) and is less likely to confuse the model. Suppose we only wanted to identify every body page in the deposition, so that we could do further processing on those pages. Let's see how this task would be setup:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Setup our binary classification task\n",
                "binary_config = ClassificationConfig(\n",
                "    # Declare the task type as 'single_label' for single-label classification\n",
                "    type='binary',\n",
                "\n",
                "    # Required for binary - Tell the model how to make the binary decision.\n",
                "    # NOTE: When providing instructions for a binary task, the default labels are 'YES' and 'NO'\n",
                "    instructions=\"Determine weather or not the page is a body page of the deposition. If the page contains the transcript or dialgoue of the deposition, select 'YES'. Otherwise, select 'NO'.\",\n",
                "\n",
                "    # Confidence score can also be requested from the model\n",
                "    # This will be returned as 'high', 'medium', or 'low' confidence\n",
                "    confidence=True\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "binary_results = await classification_provider.aprocess_document_node(\n",
                "    node,\n",
                "    task_config=binary_config\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for page_num, result in binary_results.items():\n",
                "    print(f\"Page {page_num}: {result.labels}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "That seems like it has given use the same exact response, and for a reduced token count and faster infernce time. Let's verify that our answers are indeed the same"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pages_equal = []\n",
                "for single_res, bin_res in zip(single_label_results.values(), binary_results.values()):\n",
                "    single_label = single_res.labels\n",
                "    bin_label = bin_res.labels\n",
                "\n",
                "    if single_label == \"body_page\" and bin_label == \"YES\":\n",
                "        pages_equal.append(True)\n",
                "    elif single_label == \"body_page\" and bin_label == \"NO\":\n",
                "        pages_equal.append(False)\n",
                "    elif single_label != \"body_page\" and bin_label == \"NO\":\n",
                "        pages_equal.append(True)\n",
                "    else:\n",
                "        pages_equal.append(False)\n",
                "\n",
                "assert all(pages_equal), \"Binary and single label classifications do not match\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It seems like our classification task was successful! Pooling the same classification task into multiple task configurations as is shown above, can be a very effective way of reducing errors in pipelines where incredibly high accuracy is a top priority. Thankfully DocPrompt makes this process incredibly easy!\n",
                "\n",
                "This notebook uses more code checking and displaying results than is even required to generate these results in the first place!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
