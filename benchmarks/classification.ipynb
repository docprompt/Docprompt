{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Classification\n",
    "\n",
    "A very important task for PDF and document processing. For this benchmark, we will run Docprompt Classification Task over the [Document Classification Dataset](https://www.kaggle.com/datasets/ritvik1909/document-classification-dataset/data) from Kaggle.\n",
    "\n",
    "This is a small subset of the rvl cdip dataset, containing 3 out of the 16 classes:\n",
    "- `email`\n",
    "- `resume`\n",
    "- `scientific_publication`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email: 55 images\n",
      "resume: 55 images\n",
      "scientific_publication: 55 images\n",
      "Cleaned up extracted_documents\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import shutil\n",
    "\n",
    "# Assuming the zip file is in the same directory as the notebook\n",
    "zip_file_path = '../data/benchmark/document_classifications.zip'\n",
    "output_directory = 'extracted_documents'\n",
    "\n",
    "# Dictionary to store our results\n",
    "document_data = {\n",
    "    'email': [],\n",
    "    'resume': [],\n",
    "    'scientific_publication': []\n",
    "}\n",
    "\n",
    "# Function to encode image to base64\n",
    "def encode_image(image_path):\n",
    "    with Image.open(image_path).convert('RGB') as img:\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        img.save(img_byte_arr, format='PNG', optimize=True)\n",
    "        return base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')\n",
    "\n",
    "try:\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_directory)\n",
    "\n",
    "    # Process each directory\n",
    "    for category in document_data.keys():\n",
    "        category_path = os.path.join(output_directory, category)\n",
    "        for filename in os.listdir(category_path):\n",
    "            if filename.endswith('.png'):\n",
    "                file_path = os.path.join(category_path, filename)\n",
    "                encoded_image = encode_image(file_path)\n",
    "                document_data[category].append(encoded_image)\n",
    "\n",
    "    # Print the number of images in each category\n",
    "    for category, images in document_data.items():\n",
    "        print(f\"{category}: {len(images)} images\")\n",
    "\n",
    "finally:\n",
    "    # Clean up: remove the output directory and its contents\n",
    "    if os.path.exists(output_directory):\n",
    "        shutil.rmtree(output_directory)\n",
    "        print(f\"Cleaned up {output_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at a sample image from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def decode_image(base64_string):\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "    return np.array(img)  # Convert to numpy array\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Sample Images from Each Category')\n",
    "\n",
    "for i, category in enumerate(document_data.keys()):\n",
    "    if document_data[category]:\n",
    "        img_array = decode_image(document_data[category][0])\n",
    "        axs[i].imshow(img_array)\n",
    "        axs[i].set_title(category)\n",
    "        axs[i].axis('off')\n",
    "    else:\n",
    "        axs[i].text(0.5, 0.5, f\"No images in {category}\", ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_data['email'][0][:100] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform all of the data into unique Image URIs, that we can use for running our benchmark with Anthropic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, image in document_data.items():\n",
    "    for img_i in range(len(images)):\n",
    "        # Create a base64 uri for the image\n",
    "        document_data[key][img_i] = f\"data:image/png;base64,{document_data[key][img_i]}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_data['email'][0][:100] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data prepared, we can move on to utilizing DocPrompt functionality to setup and execute our benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame to store our dataset in a tabular format\n",
    "\n",
    "rows = []\n",
    "prev_images = set()\n",
    "for key, images in document_data.items():\n",
    "    print(key)\n",
    "    image_set = set(images)\n",
    "\n",
    "    # Check for duplicate images\n",
    "    if prev_images.intersection(image_set):\n",
    "        print(f\"Duplicate images found in {key}\")\n",
    "    \n",
    "    prev_images = image_set\n",
    "    rows.extend([{'data': img, 'label': key} for img in images])\n",
    "    \n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "from docprompt.tasks.classification.base import ClassificationInput\n",
    "from docprompt.tasks.classification import anthropic\n",
    "from docprompt.utils import inference\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "importlib.reload(anthropic)\n",
    "importlib.reload(inference)\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Setup our task\n",
    "t_input = ClassificationInput(\n",
    "    type='single_label',\n",
    "    labels=['label_email', 'label_resume', 'label_scientific_publication'],\n",
    "    descriptions=[\n",
    "        \"An email or correspondance.\",\n",
    "        'A resume or CV.',\n",
    "        \"A page, abstract, or other excerpt from a scientific publication.\"\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "image_uris = [img for img in df['data']]\n",
    "\n",
    "\n",
    "results = await anthropic.classify_images(image_uris, t_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's merge the results back into our DataFrame\n",
    "\n",
    "predicted_labels = [res.labels.replace(\"label_\", \"\") for res in results]\n",
    "\n",
    "df['predicted_label'] = predicted_labels\n",
    "\n",
    "df['accuracy'] = df.apply(lambda row: int(row['label'] == row['predicted_label']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 1. Confusion Matrix (manual calculation)\n",
    "labels = df['label'].unique()\n",
    "cm = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "for true, pred in zip(df['label'], df['predicted_label']):\n",
    "    i, j = list(labels).index(true), list(labels).index(pred)\n",
    "    cm[i, j] += 1\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# 2. Accuracy by Label\n",
    "accuracy_by_label = df.groupby('label')['accuracy'].mean()\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracy_by_label.plot(kind='bar')\n",
    "plt.title('Accuracy by Label')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(accuracy_by_label):\n",
    "    plt.text(i, v + 0.01, f'{v:.2f}', ha='center')\n",
    "plt.show()\n",
    "\n",
    "# 3. Overall Accuracy Pie Chart\n",
    "overall_accuracy = df['accuracy'].mean()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie([overall_accuracy, 1 - overall_accuracy], \n",
    "        labels=['Correct', 'Incorrect'], \n",
    "        autopct='%1.1f%%', \n",
    "        colors=['#66b3ff', '#ff9999'])\n",
    "plt.title('Overall Prediction Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Print overall accuracy\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
